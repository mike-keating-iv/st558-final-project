---
title: "Modeling"
format: html
Author: Mike Keating
---

## Introduction

In our EDA of the diabetes dataset, we explored the potential impact of socioeconomic variables such as income bracket, education level, and cost barriers to medical care.

The dataset includes variables related to:

-   **Health history:** High blood pressure, high cholesterol, stroke, heart disease
-   **Lifestyle habits:** Physical activity, smoking, alcohol consumption, diet
-   **Demographics:** Sex, age, education, income

Our goal in this modeling section is to create a classification model to predict whether or not someone has diabetes. In addition to these socioeconomic variables we will include variables more directly related to health such as BMI, physical activity, and age. This will ideally improve prediction performance of our models while keeping computational cost down as opposed to using every possible predictor.

### Data

```{r warning=FALSE, message=FALSE}
# Dependencies
library(tidyverse) 
library(tidymodels)
library(ggplot2) 


# Load data
diabetes <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv", 
                     show_col_types = FALSE)
diabetes <- diabetes |> 
  mutate(Income = factor(Income, 
                         levels = 1:8, 
                         labels = c("LT $10k", "$10-15k", "$15-20k", "$20-25k", "$25-35k",
                                    "$35-50k", "$50-75k", "GT $75k")),
         Education = factor(Education,
                            levels = 1:6,
                            labels = c("None", "Elementary", "Some HS", "HS Graduate", 
                                       "Some College", "College Graduate")),
         NoDocbcCost = factor(NoDocbcCost,
                              levels = 0:1,
                              labels = c("No Barrier", "Cost Barrier")),
         Diabetes_binary = factor(Diabetes_binary,
                                  levels = 0:1,
                                  labels = c("Nondiabetic", "Diabetic")),
         PhysActivity = factor(PhysActivity,
                               levels = 0:1,
                               labels = c("No", "Yes")),
         Age = factor(Age,
                      levels = 0:13 )) |>
  select(Diabetes_binary, Income, Education, NoDocbcCost, PhysActivity, Age, BMI)
  

```

### Test Train Split

```{r}
# Set Seed
set.seed(123)

# Split into test and train sets
diabetes_split <- diabetes |> initial_split(prop=0.7)
train <- training(diabetes_split)
test <- testing(diabetes_split)
```

### Logistic Regression

Logistic regression is a generalized linear model that is used for binary classification problems. It transforms the desired response variable to a sigmoid function representing the log-odds of a success and links to the linear combination of the predictors. It is useful when classifying into one of two possibilities (for example, diabetic or nondiabetic) and is relatively simple to tune (through selection of predictors).

```{r}
# Use all preselected predictors
LR1_recipe <- recipe(Diabetes_binary ~ ., data = train) |> 
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Only socieoeconomic
LR2_recipe <- recipe(Diabetes_binary ~ Income + Education + NoDocbcCost, 
                     data = train) |>
  step_dummy(all_nominal_predictors())

# With Interaction
LR3_recipe <- recipe(
  Diabetes_binary ~ Income + Education + NoDocbcCost + BMI + PhysActivity, 
                     data = train) |> 
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_interact(terms = ~ starts_with("Edu"):starts_with("Income"))

# Spec model
LR_spec <- logistic_reg() |> 
  set_engine("glm")

# Create workflows
LR1_wkf <- workflow() |> 
  add_recipe(LR1_recipe) |> 
  add_model(LR_spec)

LR2_wkf <- workflow() |> 
  add_recipe(LR2_recipe) |> 
  add_model(LR_spec)

LR3_wkf <- workflow() |> 
  add_recipe(LR3_recipe) |> 
  add_model(LR_spec)

# Cross-validation folds
cv_folds = vfold_cv(train, 5, 2)

# Fit Models
# NOTE: Keeping 
LR1_fit <- LR1_wkf |> 
  fit_resamples(cv_folds, 
                metrics = metric_set(accuracy, mn_log_loss))

LR2_fit <- LR2_wkf |> 
  fit_resamples(cv_folds, 
                metrics = metric_set(accuracy, mn_log_loss))

LR3_fit <- LR3_wkf |> 
  fit_resamples(cv_folds, 
                metrics = metric_set(accuracy, mn_log_loss))

# Metrics 
rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics(),
      LR3_fit |> collect_metrics()) |>
  mutate(Model = c("LR1", "LR1", "LR2", "LR2","LR3", "LR3")) |>
  select(Model, everything())
```

The LR1 model (all previously selected predictors) performs the best when evaluating over log-loss, and is nearly identical to the model with interaction, suggesting adding this interaction term did not improve our model very much.

```{r}
# Save best LR model
LR_best <- LR1_wkf

```

### Classification Tree

A classification tree is a decision-based model that recursively splits the dataset into subgroups based on predictor values. Essentially, each point in the tree chooses a variable and a threshold (e.g. BMI \>30) to best separate the desired target classes (diabetic, non diabetic). This is repeated until certain criteria is met, such as tree depth, number of observations, and/or complexity penalty. Decision trees can be useful because they automatically filter for important features (feature thresholds that don't lead to information gain generally are not selected). Additionally, decision trees are easy to interpret and do not require scaling of numeric features as they do not use distance-based metrics.

```{r}
# Recipe (only need one since we will tune)
# Note that we don't need to normalize the numeric values here since tree models are not distance based
tree_recipe <- recipe(Diabetes_binary ~ ., data = train) |>
  step_dummy(all_nominal_predictors()) 

# Model
tree_spec <- decision_tree(cost_complexity = tune(),
                           tree_depth = NULL,
                           min_n = NULL) |>
  set_engine("rpart") |>
  set_mode("classification")

# Set complexity grid for tuning
tree_grid <- grid_regular(cost_complexity(), levels = 5)

# Workflow
tree_wkf <- workflow() |>
  add_model(tree_spec) |>
  add_recipe(tree_recipe)

tree_fit <- tune_grid(tree_wkf,
                      resamples = cv_folds,
                      grid = tree_grid,
                      metrics = metric_set(accuracy, mn_log_loss))

tree_fit |> collect_metrics()

```

```{r}
# Get the best model based on log loss
tree_best <- select_best(tree_fit, metric = "mn_log_loss")

tree
```

### Random Forest

### Final Model Selection
