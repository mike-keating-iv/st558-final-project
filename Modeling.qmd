---
title: "Modeling"
format: html
Author: Mike Keating
---

## Introduction

In our EDA of the diabetes dataset, we explored the potential impact of socioeconomic variables such as income bracket, education level, and cost barriers to medical care.

The dataset includes variables related to:

-   **Health history:** High blood pressure, high cholesterol, stroke, heart disease
-   **Lifestyle habits:** Physical activity, smoking, alcohol consumption, diet
-   **Demographics:** Sex, age, education, income

Our goal in this modeling section is to create a classification model to predict whether or not someone has diabetes. In addition to these socioeconomic variables we will include variables more directly related to health such as BMI, physical activity, and age. This will ideally improve prediction performance of our models while keeping computational cost down as opposed to using every possible predictor.

### Data

```{r warning=FALSE, message=FALSE}
# Dependencies
library(tidyverse) 
library(tidymodels)
library(ggplot2) 


# Load data
diabetes <- read_csv("data/diabetes_binary_health_indicators_BRFSS2015.csv", 
                     show_col_types = FALSE)

# Treat as factor and assign labels
diabetes <- diabetes |> 
  mutate(Income = factor(Income, 
                         levels = 1:8, 
                         labels = c("LT $10k", "$10-15k", 
                                    "$15-20k", "$20-25k", 
                                    "$25-35k","$35-50k", 
                                    "$50-75k", "GT $75k")),
         Education = factor(Education,
                            levels = 1:6,
                            labels = c("None", "Elementary", 
                                       "Some HS", "HS Graduate", 
                                       "Some College", "College Graduate")),
         NoDocbcCost = factor(NoDocbcCost,
                              levels = 0:1,
                              labels = c("No Barrier", "Cost Barrier")),
         Diabetes_binary = factor(Diabetes_binary,
                                  levels = 0:1,
                                  labels = c("Nondiabetic", "Diabetic")),
         PhysActivity = factor(PhysActivity,
                               levels = 0:1,
                               labels = c("No", "Yes")),
         Age = factor(Age, 
                      levels = 1:13,
                      labels = c("18-24","25-29",
                                 "30-34","35-39",
                                 "40-44","45-49",
                                 "50-54","55-59",
                                 "60-64","65-69",
                                 "70-74","75-79",
                                 "80 or older"))) |>
  select(Diabetes_binary, Income, Education, NoDocbcCost, PhysActivity, Age, BMI)
  

```

### Test Train Split

```{r warning=FALSE}
# Set Seed
set.seed(123)

# Split into test and train sets
diabetes_split <- diabetes |> initial_split(prop=0.7)
train <- training(diabetes_split)
test <- testing(diabetes_split)
library(future)


```

### Logistic Regression

Logistic regression is a generalized linear model that is used for binary classification problems. It transforms the desired response variable to a sigmoid function representing the log-odds of a success and links to the linear combination of the predictors. It is useful when classifying into one of two possibilities (for example, diabetic or nondiabetic) and is relatively simple to tune (through selection of predictors).

```{r}
# Use all preselected predictors
LR1_recipe <- recipe(Diabetes_binary ~ ., data = train) |> 
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())

# Only socieoeconomic
LR2_recipe <- recipe(Diabetes_binary ~ Income + Education + NoDocbcCost, 
                     data = train) |>
  step_dummy(all_nominal_predictors())

# With Interaction
LR3_recipe <- recipe(
  Diabetes_binary ~ Income + Education + NoDocbcCost + BMI + PhysActivity, 
                     data = train) |> 
  step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_interact(terms = ~ starts_with("Edu"):starts_with("Income"))

# Spec model
LR_spec <- logistic_reg() |> 
  set_engine("glm")

# Create workflows
LR1_wkf <- workflow() |> 
  add_recipe(LR1_recipe) |> 
  add_model(LR_spec)

LR2_wkf <- workflow() |> 
  add_recipe(LR2_recipe) |> 
  add_model(LR_spec)

LR3_wkf <- workflow() |> 
  add_recipe(LR3_recipe) |> 
  add_model(LR_spec)

# Cross-validation folds
cv_folds = vfold_cv(train, 5)

# Fit Models
# NOTE: Keeping 
LR1_fit <- LR1_wkf |> 
  fit_resamples(cv_folds, 
                metrics = metric_set(accuracy, mn_log_loss))

LR2_fit <- LR2_wkf |> 
  fit_resamples(cv_folds, 
                metrics = metric_set(accuracy, mn_log_loss))

LR3_fit <- LR3_wkf |> 
  fit_resamples(cv_folds, 
                metrics = metric_set(accuracy, mn_log_loss))

# Metrics 
rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics(),
      LR3_fit |> collect_metrics()) |>
  mutate(Model = c("LR1", "LR1", "LR2", "LR2","LR3", "LR3")) |>
  select(Model, everything())
```

The LR1 model (all previously selected predictors) performs the best when evaluating over log-loss, and is nearly identical to the model with interaction, suggesting adding this interaction term did not improve our model very much.

```{r}
# Save best model and fit on test set
LR_final_fit <- LR1_wkf |> 
  last_fit(diabetes_split, metrics = metric_set(accuracy, mn_log_loss))

LR_final_metrics <- LR_final_fit |> collect_metrics() |> mutate(Model = c("Log Reg"))
```

### Classification Tree

A classification tree is a decision-based model that recursively splits the dataset into subgroups based on predictor values. Essentially, each point in the tree chooses a variable and a threshold (e.g. BMI \>30) to best separate the desired target classes (diabetic, non diabetic). This is repeated until certain criteria is met, such as tree depth, number of observations, and/or complexity penalty. Decision trees can be useful because they automatically filter for important features (feature thresholds that don't lead to information gain generally are not selected). Additionally, decision trees are easy to interpret and do not require scaling of numeric features as they do not use distance-based metrics.

```{r}
# Recipe (only need one since we will tune)
# Note that we don't need to normalize the numeric values here since tree models are not distance based
tree_recipe <- recipe(Diabetes_binary ~ ., data = train) |>
  step_dummy(all_nominal_predictors()) 

# Model
tree_spec <- decision_tree(cost_complexity = tune(),
                           tree_depth = NULL,
                           min_n = NULL) |>
  set_engine("rpart") |>
  set_mode("classification")

# Set complexity grid for tuning
tree_grid <- grid_regular(cost_complexity(), levels = 5)

# Workflow
tree_wkf <- workflow() |>
  add_model(tree_spec) |>
  add_recipe(tree_recipe)

tree_fit <- tune_grid(tree_wkf,
                      resamples = cv_folds,
                      grid = tree_grid,
                      metrics = metric_set(accuracy, mn_log_loss))

tree_fit |> collect_metrics()

```

```{r}
# Get the best model based on log loss
tree_best_params <- select_best(tree_fit, metric = "mn_log_loss")

# Final workflow
tree_final_wkf <- tree_wkf |> 
  finalize_workflow(tree_best_params)

# Final fit
tree_final_fit <- tree_final_wkf |> 
  last_fit(diabetes_split, 
           metrics = metric_set(accuracy, mn_log_loss))

tree_final_metrics <- tree_final_fit |>
  collect_metrics() |>
  mutate(Model = c("Tree"))
```

### Random Forest

Random Forest is an ensemble method that builds multiple decision trees during training and aggregates their outputs to improve accuracy and control fitting. Each tree is trained on a random subset of the training data (bagging) as well as a random subset of features.

For classification, the final prediction is performed via a majority vote of all the ensemble trees.

```{r}
# Make a smaller sample for tuning since performance was a bottleneck
small_train <- train |> slice_sample(prop = 0.2)
small_cv_folds <- vfold_cv(small_train, v = 5)

# Recipe
rf_small_recipe <- recipe(Diabetes_binary ~ ., data = small_train) |>
  step_dummy(all_nominal_predictors())

# Random forest model
rf_spec <- rand_forest(mtry = tune(), trees = 150) |>
  set_engine("ranger") |>
  set_mode("classification")

# Workflow
rf_wkf <- workflow() |> 
  add_recipe(rf_small_recipe) |>
  add_model(rf_spec)

# Finalize mtry range
param_set <- rf_spec |> parameters()
final_param_set <- finalize(param_set, small_train)

# Build tuning grid
# Levels was hand-tuned after some trial and error
rf_grid <- grid_regular(final_param_set, levels = 7)

# Cross-validation fit
rf_fit <- rf_wkf |> 
  tune_grid(resamples = small_cv_folds,
            grid = rf_grid,
            metrics = metric_set(accuracy, mn_log_loss))

```

```{r}
# Get the best model based on log loss
rf_best_params <- select_best(rf_fit, metric = "mn_log_loss")

# Final recipe (trained on full training set)
rf_final_recipe <- recipe(Diabetes_binary ~ ., data = train) |> 
  step_dummy(all_nominal_predictors())

# Final workflow
rf_final_wkf <- workflow() |> 
  add_recipe(rf_final_recipe) |> 
  add_model(rf_spec) |> 
  finalize_workflow(rf_best_params)

# Final Fit
rf_final_fit <- rf_final_wkf |>
  last_fit(diabetes_split, 
           metrics = metric_set(accuracy, mn_log_loss))

```

```{r}
# Collect metrics
rf_final_metrics <- rf_final_fit |> 
  collect_metrics() |> 
  mutate(Model = "Random Forest")
```

### Final Model Selection

Let's take a look at the top model for each technique.

```{r}
rbind(LR_final_metrics, tree_final_metrics, rf_final_metrics) |> 
  filter(.metric == "mn_log_loss") |> 
  select(Model , everything())
```

It appears the Random Forest model performs the best when using log-loss as the score - so we will select that as our final model!

```{r}
# Let's retrain the random forest on the entire dataset for api deployment

rf_deployment_recipe <- recipe(Diabetes_binary ~ . , data = diabetes) |> 
  step_dummy(all_nominal_predictors())

# Deployment workflow
rf_deployment_wkf <- workflow() |> 
  add_recipe(rf_deployment_recipe) |> 
  add_model(rf_spec) |> 
  finalize_workflow(rf_best_params)

rf_deployment_fit <- rf_deployment_wkf |>
  fit(diabetes)


```

Let's save this into a file object so we can retrieve it in our api without retraining and refitting.

```{r}

# Save final model

write_rds(rf_deployment_fit, "final_model.rds")
```
